"""
DataManager module for tsxapipy.

This module provides the DataManager class, which manages data flow from API to application,
including authentication, data streaming, and candle aggregation.
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Callable, Optional, Any, Union, Tuple
import threading # Ensure threading is imported if df_lock is used

# Import from tsxapipy package
from tsxapipy.common.time_utils import UTC_TZ
from tsxapipy.api import schemas
from tsxapipy.api.exceptions import (
    APIError,                     # General API Error
    ValueError,     
    APIResponseParsingError,      # For issues parsing API responses
    AuthenticationError,          # For auth failures
    ConfigurationError            # For config issues
)
from tsxapipy.auth import authenticate # Assuming DataManager's initialize_components calls this
from tsxapipy.api.client import APIClient # Assuming DataManager initializes or receives an APIClient
from tsxapipy.pipeline.candle_aggregator import LiveCandleAggregator # If used
from tsxapipy.real_time.data_stream import DataStream # If used
from tsxapipy.real_time.stream_state import StreamConnectionState # If used
from tsxapipy.config import ACCOUNT_ID_TO_WATCH as DEFAULT_DM_ACCOUNT_ID # For default if needed

logger = logging.getLogger(__name__)

def _safe_authenticate():
    """Wrapper for authenticate that ensures username and password are provided."""
    import os
    from tsxapipy.auth import authenticate
    username = os.environ.get("TSX_API_USERNAME", "cay7man")
    password = os.environ.get("TSX_API_PASSWORD", "test_password")
    return authenticate(username, password)

class DataManager:
    """
    Manages data flow from API to application components.
    
    This class handles authentication, data streaming, and candle aggregation.
    """
    
    def __init__(self, supported_timeframes=None, ema_period=9, sma_period=20,account_id_for_history: Optional[int] = None):
        """
        Initialize the DataManager.
        
        Args:
            supported_timeframes: List of timeframes in seconds to support (default: [300, 900, 3600])
            ema_period: Period for EMA calculation (default: 9)
            sma_period: Period for SMA calculation (default: 20)
        """
        self.logger = logging.getLogger(__name__)
        
        self.account_id_for_history = account_id_for_history if account_id_for_history is not None else DEFAULT_DM_ACCOUNT_ID
        if self.account_id_for_history is None:
            self.logger.warning("DataManager initialized without a specific account_id_for_history. History calls might fail if accountId is required by API.")
                    
        # Set default timeframes if none provided
        if supported_timeframes is None:
            supported_timeframes = [300, 900, 3600]  # 5min, 15min, 1hr
        
        # Validate timeframes
        if not all(isinstance(tf, int) and tf > 0 for tf in supported_timeframes):
            raise ValueError("All timeframes must be positive integers")
        
        self.supported_timeframes = supported_timeframes
        
        # Set indicator periods
        self.EMA_PERIOD = ema_period
        self.SMA_PERIOD = sma_period
        
        # Initialize components to None
        self.api_client = None
        self.data_stream = None
        self.candle_aggregators = {}
        
        # State tracking
        self.is_streaming = False
        self.current_contract_id = None
        self.last_stream_status = "not_initialized"
        
        # Callbacks
        self.on_candle_update_callbacks = {}
        for tf in self.supported_timeframes:
            self.on_candle_update_callbacks[tf] = []
            
        self.on_quote_update_callbacks = []
        self.on_trade_update_callbacks = []
        self.on_stream_state_change_callbacks = []
        
        # Historical data cache
        self.historical_data = {}
        
        # Define candle DataFrame dtypes
        self.candle_dtypes = {
            'Time': 'datetime64[ns, UTC]', 
            'Open': 'float64', 
            'High': 'float64',
            'Low': 'float64', 
            'Close': 'float64',
            'Volume': 'float64'
        }
        
        # Add indicator columns to dtypes
        self.candle_dtypes[f'EMA{self.EMA_PERIOD}'] = 'float64'
        self.candle_dtypes[f'SMA{self.SMA_PERIOD}'] = 'float64'
        
        # Initialize empty DataFrames for each timeframe
        self.all_candles_dfs = {}
        for tf_sec in self.supported_timeframes:
            self.all_candles_dfs[tf_sec] = self._create_empty_candles_df()
        
        self.logger.info(f"DataManager initialized with timeframes: {self.supported_timeframes}")
        
        # Locks for thread safety
        self.df_lock = threading.RLock()
        self.latest_data_lock = threading.RLock()
        
        # Components
        self.api_client = None
        self.data_stream = None
        self.candle_aggregators = {}
        
        # Data storage
        self.all_candles_dfs = {}
        self.latest_quote = None
        self.latest_trade = None
        self.latest_quote_timestamp = None
        self.latest_trade_timestamp = None
        
        # State tracking
        self.current_contract_id = None
        self.last_stream_status = "not_initialized"
        self.is_streaming = False  # Add this attribute
        
        # Callbacks
        self.on_quote_callback = None
        self.on_trade_callback = None

        self.candle_dtypes = {
            'Time': 'datetime64[ns, UTC]', 'Open': 'float64', 'High': 'float64',
            'Low': 'float64', 'Close': 'float64',
            f'EMA{self.EMA_PERIOD}': 'float64', f'SMA{self.SMA_PERIOD}': 'float64'
        }
        for tf_sec in self.supported_timeframes:  # Changed from SUPPORTED_TIMEFRAMES_SECONDS to supported_timeframes
            self.all_candles_dfs[tf_sec] = self._create_empty_candles_df()

    def _create_empty_candles_df(self):
        """Create an empty DataFrame with the correct structure for candles."""
        df = pd.DataFrame(columns=list(self.candle_dtypes.keys()))
        for col, dtype in self.candle_dtypes.items():
            df[col] = df[col].astype(dtype)
        return df
    
    def _ensure_df_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ensures the DataFrame adheres to the predefined candle schema and dtypes."""
        # Create a new DataFrame to avoid modifying the input df if it's a view
        res_df = pd.DataFrame(index=df.index)
        for col, dtype_str in self.candle_dtypes.items():
            if col in df.columns:
                res_df[col] = df[col]
            else: # Column is missing, add it with appropriate NA type
                if 'datetime' in dtype_str:
                    res_df[col] = pd.NaT
                elif 'float' in dtype_str:
                    res_df[col] = float('nan')
                else: # Should not happen with current schema
                    res_df[col] = None # Or appropriate NA for other types
            
            # Attempt to convert to target dtype
            try:
                if col == 'Time':
                    res_df[col] = pd.to_datetime(res_df[col], errors='coerce')
                    if pd.api.types.is_datetime64_any_dtype(res_df[col]):
                        if res_df[col].dt.tz is None:
                            res_df[col] = res_df[col].dt.tz_localize('UTC')
                        else:
                            res_df[col] = res_df[col].dt.tz_convert('UTC')
                    else: # If to_datetime resulted in non-datetime (e.g., object of NaTs)
                        res_df[col] = pd.Series(res_df[col], dtype='datetime64[ns, UTC]')
                elif 'float' in dtype_str: 
                    res_df[col] = pd.to_numeric(res_df[col], errors='coerce').astype('float64')
                else: # Not expected for current schema
                    res_df[col] = res_df[col].astype(dtype_str, errors='ignore')
            except Exception as e_schema:
                self.logger.error(f"Error ensuring schema for column '{col}' to '{dtype_str}': {e_schema}. "
                                  f"Data head: {res_df[col].head() if col in res_df and not res_df.empty else 'N/A'}")
                # Fallback to NA series of the correct type if conversion fails
                if 'datetime' in dtype_str:
                    res_df[col] = pd.Series(pd.NaT, index=res_df.index, dtype='datetime64[ns, UTC]')
                elif 'float' in dtype_str:
                    res_df[col] = pd.Series(np.nan, index=res_df.index, dtype='float64')
                else:
                    res_df[col] = pd.Series(None, index=res_df.index, dtype='object') # Fallback for other types
        
        # Ensure columns are in the correct order
        return res_df[list(self.candle_dtypes.keys())]


    def _calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculates EMA and SMA indicators on the DataFrame."""
        # Ensure df is a copy to avoid SettingWithCopyWarning if it's a slice
        df_copy = df.copy()
        ema_col_name = f'EMA{self.EMA_PERIOD}'
        sma_col_name = f'SMA{self.SMA_PERIOD}'

        # Ensure Close column is numeric and handle all-NaN case
        if 'Close' not in df_copy.columns or df_copy['Close'].isnull().all():
            self.logger.debug("DataManager CalcInd: 'Close' column missing or all NaN. Indicators will be NaN.")
            df_copy[ema_col_name] = np.nan
            df_copy[sma_col_name] = np.nan
            return df_copy
        
        # Ensure 'Close' is float64 for calculations
        df_copy['Close'] = pd.to_numeric(df_copy['Close'], errors='coerce').astype('float64')

        # EMA Calculation
        # ewm().mean() correctly handles initial NaNs if min_periods is met later in the series.
        df_copy[ema_col_name] = df_copy['Close'].ewm(span=self.EMA_PERIOD, adjust=False, min_periods=self.EMA_PERIOD).mean()
        self.logger.debug(f"DataManager CalcInd: Calculated EMA{self.EMA_PERIOD}. Non-NaN count: {df_copy[ema_col_name].notna().sum()}, Tail: {df_copy[ema_col_name].dropna().tail(3).to_list() if df_copy[ema_col_name].notna().any() else 'All NaN'}")
        
        # SMA Calculation
        # rolling().mean() also handles initial NaNs correctly due to min_periods.
        df_copy[sma_col_name] = df_copy['Close'].rolling(window=self.SMA_PERIOD, min_periods=self.SMA_PERIOD).mean()
        self.logger.debug(f"DataManager CalcInd: Calculated SMA{self.SMA_PERIOD}. Non-NaN count: {df_copy[sma_col_name].notna().sum()}, Tail: {df_copy[sma_col_name].dropna().tail(3).to_list() if df_copy[sma_col_name].notna().any() else 'All NaN'}")

        return df_copy

    def initialize_components(self, contract_id: str) -> bool:
        """Initializes API client, data stream, and candle aggregators for all supported timeframes."""
        self.logger.info(f"Initializing components for Contract: {contract_id} across all supported timeframes.")
        self.current_contract_id = contract_id
        try:
            if self.data_stream and self.is_streaming:
                self.logger.info("Previously streaming. Stopping existing stream before re-initializing.")
                self.stop_streaming() # Ensure clean stop
            
            auth_result = _safe_authenticate()
            if not auth_result:
                self.logger.error("Authentication failed")
                return False
            initial_token = auth_result.get('token')
            token_acquired_at = auth_result.get('acquired_at')
            
            self.api_client = APIClient(
                username=username,
                password=password,
                initial_token=initial_token, 
                token_acquired_at=token_acquired_at
            )
            self.logger.info("APIClient initialized in DataManager.")
            
            # Initialize candle aggregators for each timeframe
            for tf in self.supported_timeframes:
                # Create callback for this timeframe
                def new_candle_callback(candle_data, timeframe=tf):
                    self._on_new_candle(candle_data, timeframe)
                
                self.candle_aggregators[tf] = LiveCandleAggregator(
                    contract_id=contract_id,
                    timeframe=tf,
                    new_candle_data_callback=new_candle_callback
                )
                self.logger.info(f"LiveCandleAggregator for {tf}s initialized.")
            
            # Initialize data stream
            self.data_stream = DataStream(
                api_client=self.api_client,
                contract_id=contract_id,
                on_quote_callback=self._handle_quote_update,
                on_trade_callback=self._handle_trade_update,
                on_error_callback=self._handle_stream_error,
                on_state_change_callback=self._handle_stream_state_change
            )
            self.logger.info("DataStream initialized in DataManager.")
            
            self.last_stream_status = "initialized"
            return True
        except (AuthenticationError, ConfigurationError, APIError) as e:
            self.logger.error(f"Failed to initialize components: {e}", exc_info=False)
            self.api_client = None
            self.data_stream = None
            self.candle_aggregators.clear()
            self.last_stream_status = f"init_error: {type(e).__name__}"
            return False
        except Exception as e_init:
            self.logger.error(f"Unexpected error during component initialization: {e_init}", exc_info=True)
            self.last_stream_status = f"unexpected_init_error: {type(e_init).__name__}"
            return False
        
    def _pass_trade_to_aggregators(self, trade_data: Dict[str, Any]):
        """Passes a single trade from DataStream to all registered candle aggregators."""
        for tf_sec, aggregator in self.candle_aggregators.items():
            try:
                aggregator.add_trade(trade_data)
            except Exception as e: # pylint: disable=broad-except
                self.logger.error(f"Error passing trade to aggregator for {tf_sec}s: {e}", exc_info=True)

    def _handle_stream_state_change(self, state_name: str):
        """
        Handles state changes from the DataStream.
        
        Args:
            state_name: The state name (string representation of StreamConnectionState enum)
        """
        self.logger.info(f"DataStream state for '{self.current_contract_id}' changed to: {state_name}")
        self.last_stream_status = state_name
        
        # Compare with the .name attribute of the enum members
        if state_name == StreamConnectionState.CONNECTED.name:
            self.is_streaming = True
        elif state_name in [
            StreamConnectionState.DISCONNECTED.name, 
            StreamConnectionState.ERROR.name,
            StreamConnectionState.RECONNECTING_TOKEN.name, 
            StreamConnectionState.RECONNECTING_UNEXPECTED.name,
            StreamConnectionState.STOPPING.name
        ]:
            self.is_streaming = False
        
        # Notify callbacks
        for callback in self.on_stream_state_change_callbacks:
            try:
                callback(state_name)
            except Exception as e:
                self.logger.error(f"Error in stream state change callback: {e}")

    def _handle_stream_error(self, error: Any):
        """Handles errors reported by the DataStream."""
        self.logger.error(f"DataStream error for '{self.current_contract_id}': {error}")
        self.is_streaming = False 
        self.last_stream_status = f"stream_error: {str(error)[:50]}" 

    def start_streaming(self) -> bool:
        """
        Start the data stream.
        
        Returns:
            bool: True if streaming started successfully, False otherwise
        """
        if not self.data_stream:
            self.logger.error("Cannot start streaming: DataStream not initialized")
            return False
        
        try:
            # Start the data stream
            self.data_stream.start()
            self.is_streaming = True
            self.last_stream_status = "streaming"
            self.logger.info("DataStream started successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to start streaming: {e}", exc_info=True)
            self.last_stream_status = f"start_error: {type(e).__name__}"
            self.is_streaming = False
            return False

    def stop_streaming(self) -> bool:
        """
        Stop the data stream.
        
        Returns:
            bool: True if streaming stopped successfully, False otherwise
        """
        if not self.data_stream:
            self.logger.warning("Cannot stop streaming: DataStream not initialized")
            return False
        
        try:
            # Stop the data stream
            self.data_stream.stop()
            self.is_streaming = False
            self.last_stream_status = "stopped"
            self.logger.info("DataStream stopped successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to stop streaming: {e}", exc_info=True)
            self.last_stream_status = f"stop_error: {type(e).__name__}"
            return False
    def _create_empty_candles_df(self) -> pd.DataFrame:
        """Creates an empty DataFrame with the correct schema for storing candle data."""
        df = pd.DataFrame(columns=list(self.candle_dtypes.keys()))
        for col, dtype_str in self.candle_dtypes.items():
            if 'datetime' in dtype_str:
                # Ensure 'Time' column is specifically datetime64[ns, UTC]
                df[col] = pd.Series(dtype='datetime64[ns, UTC]')
            elif 'float' in dtype_str:
                df[col] = pd.Series(dtype='float64') # Ensure float columns start as float64
            else: # Should not happen with current dtypes, but for robustness
                df[col] = df[col].astype(dtype_str)
        return df
    
    def _ensure_df_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ensures the DataFrame adheres to the predefined candle schema and dtypes."""
        # Create a new DataFrame to avoid modifying the input df if it's a view
        res_df = pd.DataFrame(index=df.index)
        for col, dtype_str in self.candle_dtypes.items():
            if col in df.columns:
                res_df[col] = df[col]
            else: # Column is missing, add it with appropriate NA type
                if 'datetime' in dtype_str:
                    res_df[col] = pd.NaT
                elif 'float' in dtype_str:
                    res_df[col] = float('nan')
                else: # Should not happen with current schema
                    res_df[col] = None # Or appropriate NA for other types
            
            # Attempt to convert to target dtype
            try:
                if col == 'Time':
                    res_df[col] = pd.to_datetime(res_df[col], errors='coerce')
                    if pd.api.types.is_datetime64_any_dtype(res_df[col]):
                        if res_df[col].dt.tz is None:
                            res_df[col] = res_df[col].dt.tz_localize('UTC')
                        else:
                            res_df[col] = res_df[col].dt.tz_convert('UTC')
                    else: # If to_datetime resulted in non-datetime (e.g., object of NaTs)
                        res_df[col] = pd.Series(res_df[col], dtype='datetime64[ns, UTC]')
                elif 'float' in dtype_str: 
                    res_df[col] = pd.to_numeric(res_df[col], errors='coerce').astype('float64')
                else: # Not expected for current schema
                    res_df[col] = res_df[col].astype(dtype_str, errors='ignore')
            except Exception as e_schema:
                self.logger.error(f"Error ensuring schema for column '{col}' to '{dtype_str}': {e_schema}. "
                                  f"Data head: {res_df[col].head() if col in res_df and not res_df.empty else 'N/A'}")
                # Fallback to NA series of the correct type if conversion fails
                if 'datetime' in dtype_str:
                    res_df[col] = pd.Series(pd.NaT, index=res_df.index, dtype='datetime64[ns, UTC]')
                elif 'float' in dtype_str:
                    res_df[col] = pd.Series(np.nan, index=res_df.index, dtype='float64')
                else:
                    res_df[col] = pd.Series(None, index=res_df.index, dtype='object') # Fallback for other types
        
        # Ensure columns are in the correct order
        return res_df[list(self.candle_dtypes.keys())]


    def _calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculates EMA and SMA indicators on the DataFrame."""
        # Ensure df is a copy to avoid SettingWithCopyWarning if it's a slice
        df_copy = df.copy()
        ema_col_name = f'EMA{self.EMA_PERIOD}'
        sma_col_name = f'SMA{self.SMA_PERIOD}'

        # Ensure Close column is numeric and handle all-NaN case
        if 'Close' not in df_copy.columns or df_copy['Close'].isnull().all():
            self.logger.debug("DataManager CalcInd: 'Close' column missing or all NaN. Indicators will be NaN.")
            df_copy[ema_col_name] = np.nan
            df_copy[sma_col_name] = np.nan
            return df_copy
        
        # Ensure 'Close' is float64 for calculations
        df_copy['Close'] = pd.to_numeric(df_copy['Close'], errors='coerce').astype('float64')

        # EMA Calculation
        # ewm().mean() correctly handles initial NaNs if min_periods is met later in the series.
        df_copy[ema_col_name] = df_copy['Close'].ewm(span=self.EMA_PERIOD, adjust=False, min_periods=self.EMA_PERIOD).mean()
        self.logger.debug(f"DataManager CalcInd: Calculated EMA{self.EMA_PERIOD}. Non-NaN count: {df_copy[ema_col_name].notna().sum()}, Tail: {df_copy[ema_col_name].dropna().tail(3).to_list() if df_copy[ema_col_name].notna().any() else 'All NaN'}")
        
        # SMA Calculation
        # rolling().mean() also handles initial NaNs correctly due to min_periods.
        df_copy[sma_col_name] = df_copy['Close'].rolling(window=self.SMA_PERIOD, min_periods=self.SMA_PERIOD).mean()
        self.logger.debug(f"DataManager CalcInd: Calculated SMA{self.SMA_PERIOD}. Non-NaN count: {df_copy[sma_col_name].notna().sum()}, Tail: {df_copy[sma_col_name].dropna().tail(3).to_list() if df_copy[sma_col_name].notna().any() else 'All NaN'}")

        return df_copy

    def initialize_components(self, contract_id: str) -> bool:
        """Initializes API client, data stream, and candle aggregators for all supported timeframes."""
        self.logger.info(f"Initializing components for Contract: {contract_id} across all supported timeframes.")
        self.current_contract_id = contract_id
        try:
            if self.data_stream and self.is_streaming:
                self.logger.info("Previously streaming. Stopping existing stream before re-initializing.")
                self.stop_streaming() # Ensure clean stop
            
            self.logger.debug("Authenticating for DataManager...")
            initial_token, token_acquired_at = authenticate()
            self.api_client = APIClient(initial_token=initial_token, token_acquired_at=token_acquired_at)
            self.logger.info("APIClient initialized in DataManager.")
            
            # Initialize candle aggregators for each timeframe
            self.candle_aggregators = {}
            for tf in self.supported_timeframes:
                self.candle_aggregators[tf] = LiveCandleAggregator(
                    contract_id=contract_id,
                    timeframe_seconds=tf
                )
                self.logger.info(f"LiveCandleAggregator for {tf}s initialized.")
            
            # Initialize data stream with callbacks
            try:
                self.data_stream = DataStream(
                    api_client=self.api_client,
                    contract_id=contract_id,
                    on_quote_callback=self._on_quote_received,  # Changed from on_quote to on_quote_callback
                    on_trade_callback=self._on_trade_received,  # Changed from on_trade to on_trade_callback
                    on_error_callback=self._on_stream_error,    # Changed from on_error to on_error_callback
                    on_state_change_callback=self._on_stream_state_change  # Changed from on_state_change to on_state_change_callback
                )
                self.logger.info("DataStream configured.")
            except Exception as e:
                self.logger.error(f"Unexpected error during DataStream initialization: {e}")
                raise

            self.last_stream_status = "initialized"
            return True
        except (AuthenticationError, ConfigurationError, APIError) as e:
            self.logger.error(f"Failed to initialize components: {e}", exc_info=False)
            self.api_client = None; self.data_stream = None; self.candle_aggregators.clear()
            self.last_stream_status = f"init_error: {type(e).__name__}"
            return False
        except Exception as e_init:
            self.logger.error(f"Unexpected error during component initialization: {e_init}", exc_info=True)
            self.last_stream_status = f"unexpected_init_error: {type(e_init).__name__}"
            return False
        
    def _pass_trade_to_aggregators(self, trade_data: Dict[str, Any]):
        """Passes a single trade from DataStream to all registered candle aggregators."""
        for tf_sec, aggregator in self.candle_aggregators.items():
            try:
                aggregator.add_trade(trade_data)
            except Exception as e: # pylint: disable=broad-except
                self.logger.error(f"Error passing trade to aggregator for {tf_sec}s: {e}", exc_info=True)

    def _handle_stream_state_change(self, state_name: str):
        """
        Handles state changes from the DataStream.
        
        Args:
            state_name: The state name (string representation of StreamConnectionState enum)
        """
        self.logger.info(f"DataStream state for '{self.current_contract_id}' changed to: {state_name}")
        self.last_stream_status = state_name
        
        # Compare with the .name attribute of the enum members
        if state_name == StreamConnectionState.CONNECTED.name:
            self.is_streaming = True
        elif state_name in [
            StreamConnectionState.DISCONNECTED.name, 
            StreamConnectionState.ERROR.name,
            StreamConnectionState.RECONNECTING_TOKEN.name, 
            StreamConnectionState.RECONNECTING_UNEXPECTED.name,
            StreamConnectionState.STOPPING.name
        ]:
            self.is_streaming = False
        
        # Notify callbacks
        for callback in self.on_stream_state_change_callbacks:
            try:
                callback(state_name)
            except Exception as e:
                self.logger.error(f"Error in stream state change callback: {e}")

    def _handle_stream_error(self, error: Any):
        """Handles errors reported by the DataStream."""
        self.logger.error(f"DataStream error for '{self.current_contract_id}': {error}")
        self.is_streaming = False 
        self.last_stream_status = f"stream_error: {str(error)[:50]}" 

    def start_streaming(self) -> bool:
        """
        Start the data stream.
        
        Returns:
            bool: True if streaming started successfully, False otherwise
        """
        if not self.data_stream:
            self.logger.error("Cannot start streaming: DataStream not initialized")
            return False
        
        try:
            # Start the data stream
            self.data_stream.start()
            self.is_streaming = True
            self.last_stream_status = "streaming"
            self.logger.info("DataStream started successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to start streaming: {e}", exc_info=True)
            self.last_stream_status = f"start_error: {type(e).__name__}"
            self.is_streaming = False
            return False

    def stop_streaming(self) -> bool:
        """
        Stop the data stream.
        
        Returns:
            bool: True if streaming stopped successfully, False otherwise
        """
        if not self.data_stream:
            self.logger.warning("Cannot stop streaming: DataStream not initialized")
            return False
        
        try:
            # Stop the data stream
            self.data_stream.stop()
            self.is_streaming = False
            self.last_stream_status = "stopped"
            self.logger.info("DataStream stopped successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to stop streaming: {e}", exc_info=True)
            self.last_stream_status = f"stop_error: {type(e).__name__}"
            return False

    def _handle_new_candle_data_from_aggregator(self, candle_data_series: pd.Series,
                                                is_forming_candle: bool, # Flag from aggregator
                                                timeframe_sec: int):
        if not isinstance(candle_data_series, pd.Series) or candle_data_series.empty:
            self.logger.warning(f"DM ({timeframe_sec}s): Rcvd invalid/empty candle_data_series. Skipping.")
            return

        required_ohlct_keys = ['Time', 'Open', 'High', 'Low', 'Close']
        if not all(key in candle_data_series.index for key in required_ohlct_keys):
            self.logger.warning(f"DM ({timeframe_sec}s): Candle series missing required OHLC/Time keys. Data: {candle_data_series}. Skipping.")
            return
        
        # --- Timestamp Processing ---
        try:
            candle_time_input = candle_data_series['Time']
            candle_time_pd_utc: Optional[pd.Timestamp] = None

            if isinstance(candle_time_input, datetime): # Python datetime from aggregator
                if candle_time_input.tzinfo is None: # Aggregator should send UTC
                    self.logger.debug(f"DM ({timeframe_sec}s): Localizing naive datetime from aggregator to UTC.")
                    candle_time_pd_utc = pd.Timestamp(candle_time_input, tz='UTC')
                elif str(candle_time_input.tzinfo).upper() != 'UTC': # Aggregator sent non-UTC aware
                    self.logger.debug(f"DM ({timeframe_sec}s): Converting non-UTC aware datetime from aggregator to UTC.")
                    candle_time_pd_utc = pd.Timestamp(candle_time_input).tz_convert('UTC')
                else: # Already a UTC Python datetime
                    candle_time_pd_utc = pd.Timestamp(candle_time_input) # pd.Timestamp constructor preserves tz from aware datetime
            elif isinstance(candle_time_input, pd.Timestamp): # Pandas Timestamp from aggregator
                if candle_time_input.tzinfo is None:
                    candle_time_pd_utc = candle_time_input.tz_localize('UTC', ambiguous='infer', nonexistent='NaT')
                elif str(candle_time_input.tzinfo).upper() != 'UTC':
                    candle_time_pd_utc = candle_time_input.tz_convert('UTC')
                else: # Already a UTC pandas Timestamp
                    candle_time_pd_utc = candle_time_input
            else: # Try to parse as a string, assuming UTC if naive
                self.logger.debug(f"DM ({timeframe_sec}s): Parsing timestamp string '{candle_time_input}' as UTC.")
                candle_time_pd_utc = pd.Timestamp(candle_time_input, tz='UTC')

            if pd.isna(candle_time_pd_utc): 
                raise ValueError("Timestamp became NaT after processing.")
        except Exception as e_ts:
            self.logger.error(f"DM ({timeframe_sec}s): Invalid 'Time' in candle series '{candle_data_series.get('Time')}': {e_ts}. Skipping."); return

        # --- OHLC Processing ---
        new_row_data_ohlc = {}
        for key in ['Open', 'High', 'Low', 'Close']:
            try: 
                new_row_data_ohlc[key] = float(candle_data_series[key])
                if new_row_data_ohlc[key] < 0: # Basic sanity check
                    self.logger.warning(f"DM ({timeframe_sec}s): Negative value for {key} ('{new_row_data_ohlc[key]}'). Using absolute.")
                    new_row_data_ohlc[key] = abs(new_row_data_ohlc[key])
            except (ValueError, TypeError, KeyError): 
                self.logger.error(f"DM ({timeframe_sec}s): Invalid value for {key} ('{candle_data_series.get(key)}'). Skipping."); return
        
        if new_row_data_ohlc['High'] < new_row_data_ohlc['Low']: # Sanity check
            self.logger.warning(f"DM ({timeframe_sec}s): High < Low. Swapping. H={new_row_data_ohlc['High']}, L={new_row_data_ohlc['Low']}")
            new_row_data_ohlc['High'], new_row_data_ohlc['Low'] = new_row_data_ohlc['Low'], new_row_data_ohlc['High']
        
        with self.df_lock:
            current_df = self.all_candles_dfs.get(timeframe_sec)
            # Ensure current_df is a DataFrame; if None, initialize it.
            if current_df is None:
                self.logger.warning(f"DM ({timeframe_sec}s): DataFrame was None. Initializing.")
                current_df = self._create_empty_candles_df()
                self.all_candles_dfs[timeframe_sec] = current_df

            df_to_process_this_cycle: pd.DataFrame # Explicitly define for clarity
            last_df_time_utc: Optional[pd.Timestamp] = None

            if not current_df.empty and 'Time' in current_df.columns:
                last_time_val = current_df['Time'].iloc[-1]
                if pd.notna(last_time_val):
                    # Ensure last_df_time_utc is a timezone-aware pandas Timestamp (UTC)
                    if not isinstance(last_time_val, pd.Timestamp): last_time_val = pd.Timestamp(last_time_val)
                    if last_time_val.tzinfo is None: last_df_time_utc = last_time_val.tz_localize('UTC')
                    elif str(last_time_val.tzinfo).upper() != 'UTC': last_df_time_utc = last_time_val.tz_convert('UTC')
                    else: last_df_time_utc = last_time_val
                # If last_time_val is NaT, last_df_time_utc remains None

            action_taken = "ignored_unknown_state"
            if last_df_time_utc is not None: # Existing valid last timestamp
                if candle_time_pd_utc == last_df_time_utc:
                    action_taken = "update_last_candle"
                elif candle_time_pd_utc > last_df_time_utc:
                    action_taken = "append_new_candle"
                else: # Out-of-order (older candle received)
                    action_taken = "ignore_out_of_order"
                    self.logger.warning(f"DM ({timeframe_sec}s): Rcvd out-of-order candle (New: {candle_time_pd_utc.isoformat()}, Last: {last_df_time_utc.isoformat()}). Ignoring.")
                    return
            else: # DataFrame is empty or its last 'Time' was NaT
                action_taken = "append_new_candle_to_empty_df"
                self.logger.debug(f"DM ({timeframe_sec}s): DF empty or last time NaT. Treating as new candle: {candle_time_pd_utc.isoformat()}")

            if action_taken == "update_last_candle":
                last_idx = current_df.index[-1]
                # Assign already UTC-aware pd.Timestamp directly. No tz parameter needed here.
                current_df.loc[last_idx, 'Time'] = candle_time_pd_utc
                current_df.loc[last_idx, ['Open', 'High', 'Low', 'Close']] = [new_row_data_ohlc['Open'], new_row_data_ohlc['High'], new_row_data_ohlc['Low'], new_row_data_ohlc['Close']]
                df_to_process_this_cycle = current_df # Process the modified current DataFrame
                # self.logger.debug(f"DM ({timeframe_sec}s): Updated candle {candle_time_pd_utc.isoformat()}")
            elif action_taken.startswith("append_new_candle"):
                # Create a new row as a DataFrame with the full schema
                new_single_row_df = self._create_empty_candles_df().iloc[0:0].copy() # Get schema structure
                new_single_row_df.loc[0, 'Time'] = candle_time_pd_utc
                new_single_row_df.loc[0, 'Open'] = new_row_data_ohlc['Open']
                new_single_row_df.loc[0, 'High'] = new_row_data_ohlc['High']
                new_single_row_df.loc[0, 'Low'] = new_row_data_ohlc['Low']
                new_single_row_df.loc[0, 'Close'] = new_row_data_ohlc['Close']
                # EMA/SMA columns will be NaN initially in this new row

                if current_df.empty or last_df_time_utc is None: # Truly appending to an empty or NaT-ended DF
                    df_to_process_this_cycle = new_single_row_df
                else: # Appending to a non-empty DF
                    df_to_process_this_cycle = pd.concat([current_df, new_single_row_df], ignore_index=True)
                self.logger.info(f"DM ({timeframe_sec}s): Appended new candle {candle_time_pd_utc.isoformat()}")
            else: # Should be "ignore_out_of_order" or an unexpected state
                if action_taken != "ignore_out_of_order": # Log if it's an unexpected state
                     self.logger.error(f"DM ({timeframe_sec}s): Unexpected action state '{action_taken}' for candle {candle_time_pd_utc.isoformat()}. Ignoring.")
                return

            # Recalculate indicators and ensure schema
            df_with_indicators = self._calculate_indicators(df_to_process_this_cycle)
            final_df_for_storage = self._ensure_df_schema(df_with_indicators)
            
            if len(final_df_for_storage) > self.MAX_CANDLES:
                final_df_for_storage = final_df_for_storage.iloc[-self.MAX_CANDLES:]
            
            self.all_candles_dfs[timeframe_sec] = final_df_for_storage
            # self.logger.debug(f"DM ({timeframe_sec}s): DF updated. Length: {len(final_df_for_storage)}")
                    
    def get_chart_data(self, timeframe_seconds: int) -> pd.DataFrame:
        """Returns a copy of the candle DataFrame for the specified timeframe."""
        with self.df_lock:
            df_to_return = self.all_candles_dfs.get(timeframe_seconds)
            if df_to_return is None or df_to_return.empty:
                self.logger.debug(f"No data available for timeframe {timeframe_seconds}s. Returning empty schema-correct DataFrame.")
                return self._create_empty_candles_df() # Return an empty DF with correct schema
            
            # Ensure schema and create a copy before returning to ensure thread safety for the caller
            df_copy = self._ensure_df_schema(df_to_return.copy())
            self.logger.debug(f"Returning DataFrame copy for TF {timeframe_seconds}s, len: {len(df_copy)}. "
                              f"EMA head: {df_copy[f'EMA{self.EMA_PERIOD}'].head().to_list() if f'EMA{self.EMA_PERIOD}' in df_copy and not df_copy.empty else 'N/A'}")
            return df_copy

    def update_stream_token_if_needed(self):
        """Checks if the APIClient's token needs refresh and updates the DataStream's token."""
        if self.api_client and self.data_stream:
            try:
                self.logger.info("DM: Checking/refreshing APIClient token for DataStream...")
                latest_token = self.api_client.current_token 
                
                stream_conn_status_enum = self.data_stream.connection_status # This is StreamConnectionState Enum
                self.logger.info(f"DM: Calling update_token on DataStream (current stream state: {stream_conn_status_enum.name}).")
                self.data_stream.update_token(latest_token)
            except AuthenticationError as e_auth:
                self.logger.error(f"DM: AuthenticationError during token refresh for stream: {e_auth}")
                self.last_stream_status = f"token_auth_err" 
            except APIError as e_api: 
                self.logger.error(f"DM: APIError during token refresh for stream: {e_api}")
                self.last_stream_status = f"token_api_err"
            except Exception as e_token: 
                self.logger.error(f"DM: Unexpected error updating stream token: {e_token}", exc_info=True)
                self.last_stream_status = f"token_err"
        else:
            self.logger.debug("DM: APIClient or DataStream not available for token update check.")
    
    def get_current_status_summary(self) -> str:
        """Get a summary of the current status of all components."""
        status_parts = []
        
        # Add information about the data stream
        if hasattr(self, 'data_stream') and self.data_stream:
            # Check if connection_status exists, otherwise use state attribute
            if hasattr(self.data_stream, 'connection_status'):
                status_parts.append(f"Lib Stream Conn: '{self.data_stream.connection_status.name}'")
            elif hasattr(self.data_stream, 'state'):
                status_parts.append(f"Lib Stream Conn: '{self.data_stream.state.name}'")
            else:
                status_parts.append("Lib Stream Conn: 'Unknown'")
        else:
            status_parts.append("Lib Stream Conn: 'Not initialized'")
        
        # Add information about the app stream status
        status_parts.append(f"App Stream Status: '{self.last_stream_status}'")
        
        # Add information about streaming activity
        status_parts.append("Streaming Active" if self.is_streaming else "Streaming Inactive")
        
        return " | ".join(status_parts)
    
    def _map_timeframe_to_api_params(self, timeframe_seconds_for_hist: int) -> Optional[Tuple[int, int]]:
        """Maps internal timeframe (seconds) to API `unit` and `unitNumber` for history calls."""
        if timeframe_seconds_for_hist is None: 
            self.logger.error("DM History: Timeframe not provided for API param mapping.")
            return None
            
        tf_sec = timeframe_seconds_for_hist
        if tf_sec == 1: return (1, 1)      
        if tf_sec == 60: return (2, 1)     
        if tf_sec == 300: return (2, 5)    
        if tf_sec == 900: return (2, 15)   
        if tf_sec == 1800: return (2, 30)  
        if tf_sec == 3600: return (3, 1)   
        if tf_sec == 14400: return (3, 4)  
        if tf_sec == 86400: return (4, 1)  
        
        self.logger.warning(f"DM History: Timeframe {tf_sec}s does not map directly. Attempting general mapping.")
        if tf_sec < 60 : return (1, tf_sec) 
        if tf_sec % 60 == 0 and tf_sec // 60 < 60 : return (2, tf_sec // 60) 
        if tf_sec % 3600 == 0 and tf_sec // 3600 < 24: return (3, tf_sec // 3600) 
        
        self.logger.error(f"DM History: Could not reliably map timeframe {tf_sec}s to API history parameters.")
        return None

